# ğŸ“· Real vs AI-Generated Image Detection System

## ğŸ“Œ Overview

This project implements a **computer visionâ€“based image authenticity detection system** that classifies whether an image is **real** or **AI-generated**.
The system is designed to study how different deep learning architectures perform on the task of **synthetic image detection**, an increasingly important problem in the context of generative AI.

Multiple modeling strategies are implemented and systematically compared, including:

* Custom CNN architectures trained from scratch
* Transfer learning using pre-trained **VGG16** and **ResNet50**

The emphasis of this project is on **model comparison, robustness, and deployment-aware trade-offs**, rather than relying on a single architecture.


## ğŸ¯ Objectives

* Build baseline CNN models for real vs synthetic image classification
* Apply transfer learning using ImageNet-pretrained backbones
* Compare models using standard classification metrics
* Analyze trade-offs between **accuracy, robustness, and computational cost**
* Identify a practical model choice for real-world deployment scenarios

---

## ğŸ“‚ Dataset

### ğŸ”— Source

* **CIFAKE â€“ Real and AI-Generated Synthetic Images**
* Kaggle:
  [https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images](https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images)

### ğŸ“ Dataset Structure

The dataset is automatically downloaded using `kagglehub` and organized as follows:

```text
cifake/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ REAL/
â”‚   â””â”€â”€ FAKE/
â””â”€â”€ test/
    â”œâ”€â”€ REAL/
    â””â”€â”€ FAKE/
```

* Binary classification task: **REAL vs FAKE**
* Images are low-resolution, emphasizing robustness over fine-grained details

---

## ğŸ—‚ï¸ Project Structure

```text
real-and-ai-generated-synthetic-images/
â”‚
â”œâ”€â”€ dl_cifake/
â”œâ”€â”€ runs/
â”œâ”€â”€ trained_models/
â”‚
â”œâ”€â”€ CNN_implementation_design1.ipynb
â”œâ”€â”€ CNN_implementation_design2.ipynb
â”œâ”€â”€ TransferLearning_VGG16.ipynb
â”œâ”€â”€ TransferLearning_ResNet50.ipynb
â””â”€â”€ README.md
```

Each notebook is self-contained and focuses on a specific modeling strategy.

---

## ğŸ§  Models and Methods

### ğŸ”¹ Design 1 â€” Custom CNN (PyTorch)

A lightweight CNN designed as a strong baseline.

**Key characteristics**

* 3 convolutional blocks (Conv + BatchNorm + LeakyReLU + MaxPooling)
* Fully connected layers with Dropout
* Input size: `32 Ã— 32`
* Experiments with and without data augmentation

This model prioritizes **efficiency and simplicity**, making it suitable for environments with limited computational resources.

---

### ğŸ”¹ Design 2 â€” CNN with ReLU (TensorFlow / Keras)

A CNN variant implemented using Keras.

**Key characteristics**

* Conv2D + ReLU + MaxPooling
* Global Average Pooling
* Built-in data augmentation layers
* Binary classification with Sigmoid output

This design serves as a comparative baseline to study the impact of architecture and activation choices.

---

### ğŸ”¹ Transfer Learning Models

#### âœ… VGG16

* Pre-trained on ImageNet
* Frozen backbone with fine-tuning of upper layers
* Strong performance but **high computational cost**

Notebook:

```text
TransferLearning_VGG16.ipynb
```

---

#### âœ… ResNet50

* ImageNet-pretrained ResNet50 backbone
* Fine-tuned using:

  * Data augmentation
  * Weighted loss
  * Learning rate scheduling
  * Mixed precision training
* Best overall performance among all evaluated models

Notebook:

```text
TransferLearning_ResNet50.ipynb
```

---

## âš™ï¸ Installation & Usage

### 1ï¸âƒ£ Install Dependencies

```bash
pip install torch torchvision tensorflow kagglehub scikit-learn matplotlib seaborn tqdm
```

---

### 2ï¸âƒ£ Run Experiments

Each notebook automatically:

* Downloads the dataset
* Performs preprocessing
* Trains the model
* Evaluates on the test set

Recommended execution order:

1. `CNN_implementation_design1.ipynb`
2. `CNN_implementation_design2.ipynb`
3. `TransferLearning_VGG16.ipynb`
4. `TransferLearning_ResNet50.ipynb`

---

## ğŸ“Š Experimental Results

### ğŸ”¬ Evaluation Metrics

* Accuracy
* Precision
* Recall
* F1-score

---

### ğŸ“Œ Overall Performance Comparison

| Model          | Accuracy   | Precision | Recall   | F1-score |
| -------------- | ---------- | --------- | -------- | -------- |
| Design 1 â€“ CNN | 0.9399     | 0.9401    | 0.9400   | 0.9399   |
| Design 2 â€“ CNN | 0.7993     | 0.9334    | 0.6445   | 0.7625   |
| VGG16          | 0.9351     | 0.9502    | 0.9182   | 0.9339   |
| **ResNet50**   | **0.9774** | **0.98**  | **0.98** | **0.98** |

---

## ğŸ“Š Result Analysis

* The **custom CNN (Design 1)** achieves strong performance (~94%) with relatively low computational cost, making it suitable for resource-constrained environments.
* **Design 2** performs significantly worse, particularly in recall, indicating limited robustness under augmentation and architectural constraints.
* **VGG16** delivers high accuracy but requires substantially more computation and training time.
* **ResNet50** provides the best balance between accuracy and robustness, achieving nearly **98% accuracy** with well-balanced precision and recall.

These results highlight the effectiveness of deeper residual architectures for detecting AI-generated images on the CIFAKE dataset.

---

## ğŸš€ Deployment Considerations

* **ResNet50-based model**
  Recommended when detection accuracy and robustness are the primary objectives.

* **Custom CNN (Design 1)**
  A viable alternative for scenarios with limited computational resources.

This comparison supports informed decision-making when selecting models for real-world deployment.

---

## ğŸ”® Future Work

* Evaluate generalization on newer AI-generated image datasets
* Explore frequency-domain and noise-based features
* Extend the system toward real-time or API-based deployment

---

## ğŸ‘¨â€ğŸ’» Author

**Ly Nguyen**
Purpose: Computer Vision System Development and Model Benchmarking


